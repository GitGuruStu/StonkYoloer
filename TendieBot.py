import openpyxlimport requestsfrom bs4 import BeautifulSoupimport time# Load the Excel workbookworkbook = openpyxl.load_workbook("TendieBot.xlsx")sheet = workbook.active# Specify the base URL for Google searchbase_url = "https://www.google.com/search"# Iterate through each keyword in Column Afor row in sheet.iter_rows(min_row=2, max_col=1, values_only=True):    keyword = row[0]        # Construct the search query    query = "{} news".format(keyword)    params = {"q": query, "tbs": "qdr:d"}  # Search for news in the last 24 hours        # Send the request and parse the search results    response = requests.get(base_url, params=params)    soup = BeautifulSoup(response.content, "html.parser")        # Find all search result links    search_results = soup.find_all("div", class_="tF2Cxc")        if search_results:        headline_link = None                # Iterate through search results and find the first valid headline link        for result in search_results:            link = result.find("a")            if link and link.get("href") and link.get("href").startswith("http"):                headline_link = link.get("href")                break                if headline_link:            # Open the article and fetch its content            article_response = requests.get(headline_link)            article_soup = BeautifulSoup(article_response.content, "html.parser")            article_text = article_soup.get_text()                        # Update the Excel sheet with the article text            sheet.cell(row=row[0].row, column=2, value=article_text)        else:            print(f"No valid news link found for '{keyword}'")    else:        print(f"No search results found for '{keyword}'")        # Save the Excel workbook after processing each keyword    workbook.save("TendieBot.xlsx")        # Add a delay to avoid overloading the server    time.sleep(5)print("Automation completed.")